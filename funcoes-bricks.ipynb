{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função que Grava o Dataframe no Data Lake criando uma Delta Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def storeInBlobDatabricks(df,path,schema,tabela,format_write,modo):\n",
    "    try:\n",
    "        df = df.withColumn(\"dt_folder\", current_date().cast(\"date\"))\n",
    "        (df.write.format(format_write)\n",
    "        .mode(modo)\n",
    "        .partitionBy('dt_folder')\n",
    "        .option(\"mergeSchema\", \"true\")\n",
    "        .option(\"path\", path)\n",
    "        .saveAsTable(schema+tabela))\n",
    "        return True\n",
    "    except Exception as ex:\n",
    "        return ex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função que remove os espaços de todos os campos tipo string no Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RemovSpaces(df):\n",
    "    df_tr = (\n",
    "        df.select(\n",
    "            *(trim(col(colu)).alias(colu) for colu, ty in df.dtypes if ty.split('(')[0] in ('string')),\n",
    "            *(col(lun).alias(lun)for lun, tpy in df.dtypes if tpy.split('(')[0] != 'string')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return df_tr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função que Grava o dataframe no Synapse por PolyBase, utilizando como método de autenticação Managed Service Identity seguindo as práticas de segurança - KeyVault\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grava_no_dw(df, tabela, dw_conexao, temp):\n",
    "    spark.conf.set(\"fs.azure.account.auth.type.meudatalake.dfs.core.windows.net\", \"OAuth\")\n",
    "    spark.conf.set(\"fs.azure.account.oauth.provider.type.meudatalake.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "    spark.conf.set(\"fs.azure.account.oauth2.client.id.meudatalake.dfs.core.windows.net\", dbutils.secrets.get(scope = \"key-vault\", key = \"clientid\"))\n",
    "    spark.conf.set(\"fs.azure.account.oauth2.client.secret.meudatalake.dfs.core.windows.net\", dbutils.secrets.get(scope = \"key-vault\", key = \"secret\"))\n",
    "    spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.meudatalake.dfs.core.windows.net\", \"https://login.microsoftonline.com/\"+dbutils.secrets.get(scope = \"key-vault\", key = \"DiretorioId\")+\"/oauth2/token\")\n",
    "    \n",
    "    df.write.format(\"com.databricks.spark.sqldw\")\n",
    "    .option(\"url\", dw_conexao)\n",
    "    .option(\"useAzureMSI\", \"true\")\n",
    "    .option(\"dbTable\", tabela)\n",
    "    .mode(\"append\")\n",
    "    .option(\"tempDir\", temp)\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
